{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Dataset with Image Paths\n",
    "##### Update the dataset path and use ImageFolder to load the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['3D Design', 'AI-abstract Imagery', 'Asymmetry', 'Augmented Reality', 'Brutalism', 'Card-based Design', 'City', 'Dark Mode', 'Flat Design', 'Glass Morphism', 'Gradiant Design', 'Illustrative Design', 'Interactive Gradients', 'Material Design', 'Memphis Design', 'MetroUI', 'Minimalism', 'Neomorphism', 'Parallax Scrolling', 'Responsive Design', 'RetroVintage Design', 'Skeuomorphism', 'Typography-focused']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\I'\n",
      "C:\\Users\\dammi\\AppData\\Local\\Temp\\ipykernel_20680\\3050682020.py:5: SyntaxWarning: invalid escape sequence '\\I'\n",
      "  dataset_path = \"C:\\ICT Project 2024\\Image\"\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"C:\\ICT Project 2024\\Image\"\n",
    "\n",
    "# Define image transformations (resize, normalize, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to match CLIP preprocessing\n",
    "])\n",
    "\n",
    "# Load dataset using ImageFolder\n",
    "train_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 22  # Adjust as needed\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Print class names to verify\n",
    "print(f\"Classes: {train_dataset.classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load CLIP and the tokenizer / Import Required Libraries / Update Classifier with Number of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Update the classifier to match the number of design styles\n",
    "num_classes = len(train_dataset.classes)  # Automatically counts subfolders\n",
    "classifier = nn.Linear(model.visual.output_dim, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix (Define the Loss Function (criterion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 3.2375\n",
      "Epoch 2/50, Loss: 3.2199\n",
      "Epoch 3/50, Loss: 3.1955\n",
      "Epoch 4/50, Loss: 3.1847\n",
      "Epoch 5/50, Loss: 3.1638\n",
      "Epoch 6/50, Loss: 3.1430\n",
      "Epoch 7/50, Loss: 3.1261\n",
      "Epoch 8/50, Loss: 3.1088\n",
      "Epoch 9/50, Loss: 3.0914\n",
      "Epoch 10/50, Loss: 3.0681\n",
      "Epoch 11/50, Loss: 3.0511\n",
      "Epoch 12/50, Loss: 3.0321\n",
      "Epoch 13/50, Loss: 3.0213\n",
      "Epoch 14/50, Loss: 2.9922\n",
      "Epoch 15/50, Loss: 2.9823\n",
      "Epoch 16/50, Loss: 2.9607\n",
      "Epoch 17/50, Loss: 2.9492\n",
      "Epoch 18/50, Loss: 2.9228\n",
      "Epoch 19/50, Loss: 2.9075\n",
      "Epoch 20/50, Loss: 2.8816\n",
      "Epoch 21/50, Loss: 2.8741\n",
      "Epoch 22/50, Loss: 2.8573\n",
      "Epoch 23/50, Loss: 2.8413\n",
      "Epoch 24/50, Loss: 2.8198\n",
      "Epoch 25/50, Loss: 2.8058\n",
      "Epoch 26/50, Loss: 2.7943\n",
      "Epoch 27/50, Loss: 2.7626\n",
      "Epoch 28/50, Loss: 2.7607\n",
      "Epoch 29/50, Loss: 2.7350\n",
      "Epoch 30/50, Loss: 2.7249\n",
      "Epoch 31/50, Loss: 2.7066\n",
      "Epoch 32/50, Loss: 2.7002\n",
      "Epoch 33/50, Loss: 2.6724\n",
      "Epoch 34/50, Loss: 2.6508\n",
      "Epoch 35/50, Loss: 2.6375\n",
      "Epoch 36/50, Loss: 2.6177\n",
      "Epoch 37/50, Loss: 2.6031\n",
      "Epoch 38/50, Loss: 2.5963\n",
      "Epoch 39/50, Loss: 2.5764\n",
      "Epoch 40/50, Loss: 2.5567\n",
      "Epoch 41/50, Loss: 2.5354\n",
      "Epoch 42/50, Loss: 2.5270\n",
      "Epoch 43/50, Loss: 2.5035\n",
      "Epoch 44/50, Loss: 2.4864\n",
      "Epoch 45/50, Loss: 2.4781\n",
      "Epoch 46/50, Loss: 2.4497\n",
      "Epoch 47/50, Loss: 2.4585\n",
      "Epoch 48/50, Loss: 2.4425\n",
      "Epoch 49/50, Loss: 2.4151\n",
      "Epoch 50/50, Loss: 2.4006\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim  # Import necessary modules\n",
    "\n",
    "# Freeze CLIP encoder\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Add a custom classification layer (already defined earlier)\n",
    "# classifier = nn.Linear(model.visual.output_dim, num_classes).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Define the loss function\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=1e-4)  # Define optimizer\n",
    "\n",
    "# Define the training loop\n",
    "epochs = 50  # Number of training epochs\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in dataloader:  # Use DataLoader for batches\n",
    "        # Move data to the device (CPU/GPU)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Extract features using the frozen CLIP encoder\n",
    "        with torch.no_grad():\n",
    "            features = model.encode_image(images)\n",
    "        \n",
    "        # Forward pass through the classifier\n",
    "        predictions = classifier(features)\n",
    "        loss = criterion(predictions, labels)  # Compute the loss\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Print loss for each epoch\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Model & Re-load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), \"style_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dammi\\AppData\\Local\\Temp\\ipykernel_20680\\3350549794.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classifier.load_state_dict(torch.load(\"style_classifier.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=23, bias=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.load_state_dict(torch.load(\"style_classifier.pth\"))\n",
    "classifier.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Design Styles as Text Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define design style prompts\n",
    "design_styles = [\n",
    "    \"Flat Design\",\n",
    "    \"Metro UI\",\n",
    "    \"Skeuomorphism\",\n",
    "    \"Material Design\",\n",
    "    \"Neomorphism\",\n",
    "    \"Brutalism\",\n",
    "    \"Responsive Design\",\n",
    "    \"Minimalism\",\n",
    "    \"Dark Mode\",\n",
    "    \"Glass Morphism\",\n",
    "    \"Gradiant Design\",\n",
    "    \"3D Design\",\n",
    "    \"RetroVintage Design\",\n",
    "    \"Asymmetry\",\n",
    "    \"Card-based Design\",\n",
    "    \"Parallax Scrolling\",\n",
    "    \"Typography-focused\",\n",
    "    \"Augmented Reality\",\n",
    "    \"Illustrative Design\",\n",
    "    \"Interactive Gradients\",\n",
    "    \"AI-abstract Imagery\",\n",
    "    \"Memphis Design\",\n",
    "]\n",
    "\n",
    "# Tokenize prompts\n",
    "text_inputs = clip.tokenize(design_styles).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and Preprocess the Image,  Calculate Similarities , Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load and preprocess image\u001b[39;00m\n\u001b[0;32m      5\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mICT Project 2024\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mImages\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTurku.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Path to your website screenshot\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m image \u001b[38;5;241m=\u001b[39m preprocess(Image\u001b[38;5;241m.\u001b[39mopen(image_path))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Generate image and text embeddings\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Load and preprocess image\n",
    "image_path = r\"C:\\ICT Project 2024\\Images\\City\\Turku.jpg\"  # Path to your website screenshot\n",
    "image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "# Generate image and text embeddings\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "# Normalize features\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity = (image_features @ text_features.T).squeeze(0)\n",
    "\n",
    "# Get top match\n",
    "top_idx = similarity.argmax().item()\n",
    "print(f\"The website design style is most likely: {design_styles[top_idx]}\")\n",
    "\n",
    "# Optionally, print all scores\n",
    "scores = {design_styles[i]: similarity[i].item() for i in range(len(design_styles))}\n",
    "print(\"All similarity scores:\", scores)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(Image.open(image_path))  # Display the image\n",
    "plt.axis('off')  # Remove axes for better display\n",
    "plt.title(f\"Predicted Style: {design_styles[top_idx]}\\nScore: {similarity[top_idx].item():.4f}\", fontsize=14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SolutionsInPR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
